# Digital Advertising Emissions Data Pipeline

A comprehensive data engineering stack for processing digital advertising emissions data using **Apache Spark**, **DBT**, **MinIO**, **Airflow**, and **Docker Compose**. This project provides both traditional Spark-based and modern SQL-first DBT-based data pipelines for local development and prototyping.

## ğŸ—ï¸ Architecture

### Tech Stack
- **Apache Spark**: Distributed data processing engine
- **DBT (Data Build Tool)**: SQL-first transformation tool
- **MinIO**: S3-compatible object storage
- **Apache Airflow**: Workflow orchestration
- **PostgreSQL**: Airflow metadata database
- **Docker & Docker Compose**: Containerization

### Data Pipeline Overview
The project implements two parallel data processing approaches:

1. **Spark Pipeline**: Traditional PySpark-based ETL pipeline
2. **DBT Pipeline**: Modern SQL-first transformation pipeline

Both pipelines process advertising emissions data through ingestion, staging, transformation, and business logic implementation stages.

## ğŸ“ Project Structure

```
digital-advertising-sample-pipeline/
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â”œâ”€â”€ advertising_emissions_pipeline.py      # Spark-based DAG
â”‚       â””â”€â”€ advertising_emissions_dbt_pipeline.py  # DBT-based DAG
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ jobs/
â”‚   â”‚   â”œâ”€â”€ ingest_data.py           # Data ingestion
â”‚   â”‚   â”œâ”€â”€ stage_data.py            # Data staging & cleaning
â”‚   â”‚   â”œâ”€â”€ transform_data.py        # Data transformation
â”‚   â”‚   â”œâ”€â”€ final_output.py          # Business logic implementation
â”‚   â”‚   â””â”€â”€ test_business_model.py   # Data quality tests
â”‚   â”œâ”€â”€ conf/                        # Spark configuration
â”‚   â””â”€â”€ init/                        # Initialization scripts
â”œâ”€â”€ dbt_project/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/                 # Staging models
â”‚   â”‚   â”œâ”€â”€ marts/                   # Data marts
â”‚   â”‚   â””â”€â”€ business_logic/          # Business logic models
â”‚   â”œâ”€â”€ tests/                       # DBT tests
â”‚   â”œâ”€â”€ docs/                        # Documentation
â”‚   â””â”€â”€ Dockerfile                   # DBT container image
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚       â””â”€â”€ advertising_emissions.csv  # Sample dataset
â”œâ”€â”€ scripts/                         # Utility scripts
â”œâ”€â”€ docker-compose.yml               # Service definitions
â”œâ”€â”€ create-stack.sh                  # Stack setup script
â””â”€â”€ clean-stack.sh                   # Stack cleanup script
```

## ğŸš€ Quick Start

### Prerequisites
- [Docker](https://www.docker.com/get-started) and [Docker Compose](https://docs.docker.com/compose/)

### 1. Clone and Setup
```bash
git clone <your-repo-url>
cd digital-advertising-sample-pipeline
```

### 2. Clean Start (Recommended)
```bash
./clean-stack.sh
```

### 3. Create and Start Services
```bash
./create-stack.sh
```

This will:
- Start all Docker services
- Initialize MinIO with sample data
- Download required Spark/Hadoop JARs
- Set up Airflow with default admin user

### 4. Access Services

| Service | URL | Credentials |
|---------|-----|-------------|
| **MinIO Console** | [http://localhost:9001](http://localhost:9001) | `minioadmin` / `minioadmin` |
| **Spark Master UI** | [http://localhost:8080](http://localhost:8080) | - |
| **Airflow UI** | [http://localhost:8081](http://localhost:8081) | `admin` / `admin` |

âš ï¸ **Note**: Airflow may take 3-5 minutes to fully initialize after first startup.

## ğŸ“Š Data Pipelines

### Spark Pipeline (`advertising_emissions_pipeline`)
Traditional PySpark-based pipeline with the following stages:

1. **Data Ingestion**: Read CSV and convert to Parquet
2. **Data Staging**: Clean and standardize data
3. **Data Transformation**: Apply business transformations
4. **Business Logic**: Calculate metrics and aggregations
5. **Data Quality Testing**: Validate outputs

**Data Flow**:
```
s3a://sample-bucket/raw/ â†’ s3a://sample-bucket/staging/ â†’ s3a://sample-bucket/output/
```

### DBT Pipeline (`advertising_emissions_dbt_pipeline`)
Modern SQL-first pipeline using DBT for transformations:

1. **Data Ingestion**: Ingest to DBT-specific raw folder
2. **Data Staging**: DBT staging models
3. **DBT Build**: Run all models, tests, and documentation
4. **Documentation**: Generate DBT docs

**Data Flow**:
```
s3a://sample-bucket/dbt/raw/ â†’ s3a://sample-bucket/dbt/staging/ â†’ DBT Models
```

### Key Features
- **Top Domains Analysis**: Identify highest emitting domains
- **Trend Analysis**: Time-series emission patterns
- **Data Quality Checks**: Comprehensive validation rules
- **Outlier Detection**: Statistical anomaly identification
- **Emission Metrics**: Carbon footprint calculations

## ğŸ§ª Testing & Validation

### Data Quality Checks
Both pipelines include comprehensive data quality validation:
- **Completeness**: Null value detection
- **Accuracy**: Data type and range validation
- **Consistency**: Cross-field validation
- **Timeliness**: Date range checks

### Running Tests
```bash
# Spark-based tests
docker exec spark-master spark-submit /opt/bitnami/spark/jobs/test_business_model.py

# DBT tests
docker exec dbt dbt test
```

## ğŸ› Troubleshooting

### Common Issues

**Spark Jobs Failing**
- Check S3A JARs are present: `docker exec spark-master ls /opt/bitnami/spark/jars/`
- Verify MinIO connectivity: Test bucket access via MinIO console

**DBT Connection Issues**
- Ensure Spark master is running: `docker ps | grep spark-master`
- Check DBT logs: `docker logs dbt`

**Airflow DAGs Not Appearing**
- Check DAG syntax: `docker exec airflow-webserver airflow dags list`
- Review scheduler logs: `docker logs airflow-scheduler`

**Storage Issues**
- Monitor MinIO usage: Check console at [http://localhost:9001](http://localhost:9001)
- Clean old data: Use MinIO console or `mc` commands

### Log Access
```bash
# Service logs
docker logs <container-name>

# Follow logs in real-time
docker logs -f <container-name>

# Airflow task logs available in UI
```

## ğŸ”„ Data Separation

The project maintains separate data paths for each pipeline:
- **Spark Pipeline**: `s3a://sample-bucket/{raw,staging,output}/`
- **DBT Pipeline**: `s3a://sample-bucket/dbt/{raw,staging}/`

This ensures no interference between pipeline runs and allows independent development.

## ğŸ“š Additional Resources

- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [DBT Documentation](https://docs.getdbt.com/)
- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [MinIO Documentation](https://min.io/docs/)

## ğŸ§¹ Cleanup

To completely reset the environment:
```bash
./clean-stack.sh
```

This safely removes all project-related Docker resources without affecting other projects.

---

**Note**: This stack is designed for local development and prototyping. For production use, additional security, scalability, and monitoring considerations are required. 